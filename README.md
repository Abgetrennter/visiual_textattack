# **基于决策的黑盒文本对抗样本攻击方案研究与实现**

## 摘要

## 绪论

### 背景和意义

### 研究现状

###  主要研究内容

#### 动态灵活

## 正文
### 引言

深度神经网络最新取得的进展在诸如文本分类[64,65]、神经机器翻译[66]以及问答[67]等一些长期研究领域取得了重大突破，使其被广泛应用于现实世界中的许多重要任务。例如，基于深度神经网络的文本理解已成为当今社交媒体上在线信息提取和分析的核心技术。深文本分类模型也被广泛用于改善在线交流环境，例如自动检查诸如辱骂、色情、暴力等有害评论[44,68]。尽管深度神经网络在各种任务中展现出了最先进的性能，但众所周知，它们也容易受到对抗样例的攻击-即通过向正常样本中添加难以察觉的扰动来构造恶意的对抗性样例来欺骗标模型[2,3,42]。这种现象首先发现于图像分类任务，近年来其在文本领域受到了极大关注[20,51]。考虑到深度神经网络是许多现实世界中对安全敏感的应用（例如有毒内容检测和基于文本反垃圾）的核心组件，对抗样例的存在自然会引起人们极大的关注。这些担忧促使人们对生成对抗性文本进行深入研究，以进一步探索深层文本理解系统的脆弱性，进而提出提高其鲁棒性的对策。因此，在不同的场景中提出了大量的文本对抗攻击。在白盒场景中，攻击者被假设可以访问标模型的结构和参数等完整信息[19-21]。因此，攻击者可以利用模型的梯度信息来指导对抗攻击，然而，尽管这些攻击具有开创性，但它们在实践中受到限制，因为在许多实际情况下，关于完全访问标神经网络模型的基本假设并不成立。在黑盒场景中，攻击者被假设即使在无法获取标模型具体的结构和参数信息情况下，依然可以利用标模型返回的置信度信息进行对抗攻击[27,69]。该假设背后的直观原因是，置信度信息在一定程度上反映了标神经网络模型的最终决策边界，因而可以用来估计攻击所需的梯度信息[7]。然而，由于需要对标模型进行大量的查询，因而这类攻击在实践中也受到限制，因为生成一个对抗样例所需的查询次数直接决定了该攻击所造成威胁的严重性。因此，我们亟需研究基于模型决策的文本对抗攻击技术。
此外，现有的工作主要集中在针对面向英文的自然语言处理系统生成对抗文本。由于以下原因，前尚未有任何尝试通过对抗攻击来评估中文自然语言处理系统的鲁棒性的研

攻击原理

给定一个输入特征空间X，包含所有可能的输入文本（向量形式x）和一个输出空间 Y={y1,y2,....,y}，包含x的K个可能标签。,yK}包含X的K个可能的标签，分类器F需要从输入样本x∈X中学习一个映射f:X→Y
到一个正确的标签y true∈Y。在下文中，我们首先给出自然语言分类中对抗性例子的定义，然后介绍我们的词语替换策给定一个训练有素的自然语言分类器F，它可以根据最大的后发概率将原始输入文本x正确
分类为标签ytrue。如下。假设当前的输入样本属于ytrue，字典中的Dytrue⊆D包含所有出现在的NE。级别为y的文本true，我们可以使用最自由的频繁出现的命名实体NEadv。argmaxP(yi|x)=y




攻击选择模块

本论文分别采用了随机选取和关键性作为攻击字符的选择方法.其中随机选取就是从所有的可攻击汉字中按照给定的概率随机选取,但是这样不能够充分利用黑箱攻击模型给出的输出类别和每个类别的预测值去减少次数。因此,一般会使用字关键性公式去计算每个字对整个句子感情倾向的影响.该公式通过利用黑盒模型给出的信息衡量了句子中每个字对整体的影响.
$$
Score(Char_i)=F_{score}(Sentence)-F_{score}(Sentence\ without\ Char_i)
$$
但是如果单纯的使用字关键性公式计算每个字的得分,需要的查询次数太多,运行时间太长.因此本文对此加以改进,首先计算词关键性,然后使用步进随机的方法获得要求的汉字.由于句子中的某个词可能在目标模型的判断中起决定性作用，该词中的所有字符就都被安排在字符重要性列表的前面，这在改变时更容易误导目标模型。在步进选取后，所有重要的字都会被替换。对于一条输入的句子,首先使用 Thulac ^分词模块对词语进行切分,得到分词之后的结果.
$$
WordList[1..n]=CutSentence(Sentence)
$$
然后利用词语的关键性计算公式对词语进行排序.词语关键性计算公式和字关键性类似.
$$
Score(Word_i)=F_{score}(Sentence)-F_{score}(Sentence\ without\ Word_i)
$$

最后,将这些词语按照得分大小排列合并,去除非汉字部分,得到候选字列表.
$$
CharList[1..m]=Collect(WordsList[1..n],\ Sort\ by\ Score,\ Drop\ Not\ Hanzi)
$$
为了让对原文本的修改尽可能小,选择模块选择攻击的汉字数目的变化阈值会从1开始,随着攻击次数的增加而逐步增加.同时为了保证攻击的随机性,选择模块构造了一个选择列表.该列表的增长速度略快于实际选择的汉字数目.模块会在该列表中随机选取实际攻击的汉字.
$$
\begin{cases}
  SelectCount=F^1(Attack\_times) \\
  End=F^2(Attack\_times) 
 \end{cases}
  \ \ \ \ \ \ 1\leq F^1(x) < F^2(x)\leq m
$$

$$
SelectCharList=Random(CharList[1..End])[1..SelectCount]
$$

B.候选人产生过程
候选词生成模块的主要建议是通过多模态攻击，为重要性列表中的当前字符生成一个字形相似度列表、一个语音相似度列表和一个语义相似度列表。多模式攻击中每种攻击的具体算法已在第3节中描述。为了将三种多模态攻击结合在一起，本文设置了不同的权重，通过自动搜索的方法找到使生成的对抗性例子具有最高的攻击成功率和最接近的字形、语音和语义的距离的权重参数。每个权重都乘以每种多模态攻击计算的相似度，得到最终的相似度分数。在生成三种攻击方式对应的相似度列表后，对其进行统一排序，最后得到候选词列表

### 以汉字形态学为核心的递归结构拆解

与英文字母表的26个字母相比,汉字有10万多个，其中被广泛使用的有4500个，覆盖了99%的使用率。从形态学的角度讲,汉字与英文中的单词更为类似,都是由简单的部件拼接而成.因此对于英语的词语级攻击可以相应的迁移到对汉字的攻击上来.比如说,对英语单词的拼写错误攻击,近义词攻击和插入攻击,可以相应的迁移成对汉字的同义字,形近字和拆解攻击.

尤其是,汉字本身作为一种二维图像,具有一定的形态特征.从组成上看,汉字由更小的部件--偏旁部首组成,如:"亻","殳","糸,"雨","弓","骨","艹","门","凵"等;从空间上看,汉字具有一定的结构如独体,左右,上下,左中右,上中下,右上包围,左上包围,左下包围,交叉等,分别对应Unicode相应的结构字符:⿰,⿱,⿲,⿳,⿴,⿵,⿶,⿷,⿸,⿹,⿺,⿻.从这个角度讲,汉字在视觉层面上具有更为丰富的信息.此外,汉字经常会使用假借等手段.即对于一个汉字,可能有数个和其声旁一致的汉字的意思和其相同.还有会意,对于不认识的生僻字,人们往往会通过认半边的方式猜测其含义.因而,利用汉字的形态学对文本发起视觉攻击,具有运行速度快,隐蔽性高,难以应对的特点.

对中文使用视觉攻击法的好处是，单个汉字由于其复杂的笔画和部⾸，已经包含了⼤量的信息。如果文本的某些部分发生了变化，⼈类仍然可以通过对汉字的经验和理解来正确理解语义。由于复杂的笔画和部⾸，⼀个汉字已经包含了⼤量的信息。如果文本的某些部分发生了变化，⼈类仍然可以通过对汉字的经验和理解来正确理解语义。由于复杂的笔画和部⾸，⼀个汉字已经包含了⼤量的信息。如果文本的某些部分发生了变化，⼈类仍然可以通过对汉字的经验和理解来正确理解语义。

汉字结构

##### 三元组结构(C1,C2,结构)

##### 汉字结构二叉树

为了充分利用汉字的空间特征,就势必要对其进行分解储存.为了方便存储,前人多使用前缀表达式的形式对汉字进行拆解.如{新型汉字相似度计算},仓颉,以及Unicode使用IDS序列对汉字进行分解,即利用(0x忘了)的汉字结构描述符将汉字拆分成偏旁和描述符组成的前缀表达式.本文在前人的基础上,创新的将汉字进行了递归二分.采用汉字结构二叉树的方式表示汉字.这样做的好处是能够一一对应的计算汉字的相似度,同时保留了汉字的空间结构,作为整个实验的核心支撑了所有攻击方法的实现.

对于汉字来说,其组成部分可能是能够独立成字的偏旁(如"海",由"⺡"和"每"组成),也可能是不能独立成字的部件组合(如"侵",由"亻"和"彐冖又"组成).对于不能成字的部件组合,我们用一个字符元组来表示,即("彐","冖","又").因此,规定单字符和字符元组为汉字C.对于每个汉字C,都可以通过直接查询或者对其所有部件笔画数进行求和的方法求出对应的笔画数Count(C).同样的,能够获取其对应的结构Structure(C).

为了方便表示汉字的结构,定义一个枚举类型 HanziStructure 储存所有的结构,其中字符元组的结构记为专门的组合类型.枚举中每个结构根据他们的相似程度依次分配了相应的数字.可以通过一定的算法比较两个汉字的结构相似程度.

简易的,我们定义其汉字结构相似程度计算函数cal_stru_sim为1加上两个字结构枚举具体值差的绝对值.例如:枚举类型左右结构的值为1,左中右结构的值为2.则两个结构的相似程度为
$$
cal\_stru\_sim(左右,左中右)=\left |1-2  \right |+1=2
$$



对于单字符汉字,考察其结构,若其为独体结构,偏旁或者基本笔画,则停止分解,将其作为叶子节点.否则,按照其结构对字符进行二分.若其分解出的部件多于2个,则选取第一个为左节点,剩下的部分作为字符元组作为右节点.

对于字符元组,则继续选取第一个为左节点,其余的作为右节点继续递归下去.这样既保持了字符拆解序列的有序性,又^.

[汉字拆解的伪代码]
![二叉树](C:U+sers\abget\Study\黑盒攻击\代码部分\汉字拆分结构.png)



#### 汉字拆分

##### 基于汉字结构二叉树进行拆分,只选取左右部分进行拆分

考虑到汉字中绝大多数是左右结构,比如说"的"是由"白","勺"组成.将其进行横向拆解,人脑能够自动将其进行组合,对文本阅读的影响程度较小.

对于要攻击的汉字,广度优先遍历其结构二叉树.将结构为左右或者左中右等横向可拆分的节点作为遍历节点,将其余的节点作为叶子节点,这样出队序列就是汉字的拆分序列.考虑到组合结构的复杂性,为了简便起见在遇到组合节点的时候认为汉字不可横向拆分,停止遍历.

```Python
def char_flatten(c:Hanzi)->str:
	ret = ""
	char_queue = list(c)
	while char_queue:
		char = char_queue.pop(0)
		switch char.struct:
			case HanziStructure.左右 or HanziStructure.左中右:
				char_queue.append(c.sub)
			case HanziStructure.组合:
				ret = c.c #Stop Flatten Char
				break
			case default:
				ret = ret + c.c
	return ret
```

#### 动态搜索异体字

##### 基于汉字递归结构搜索.优先使用增加偏旁的汉字,其次使用汉字的部分组件.

基于汉字读半边的常见现象,将汉字加上一些偏旁对整体的影响不是很大.如"知"加上"木"形成的"椥"字.因此,基于部首构建了能够根据偏旁查询其更高一级组成的汉字的反向字典.在生成对应异体字的时候,将原汉字作为偏旁在反向字典中进行搜索,得到相应的汉字.若原汉字不作为部件组成汉字,则尝试去掉其偏旁.如"询"去掉"⻈"变成的"旬"依然在一定程度上能够保持其原来的含义.搜索完毕后之后根据笔画近似算法对结果进行排序.
$$
MarsCharList[1..n]
$$


为了让搜索到的异体字更加的接近原来的字体,需要让其余的部分尽可能的简单,占比尽可能的小.因此,首先计算剩余部分在整体字中的占比.对于增加偏旁的异体字来说,由于汉字的部件增加了,笔画数也随之增加.因此其得分为正.对于减少偏旁的异体字来说,相应的,其得分为负.

$$
score(C_{mars},C_{origin})=\frac{Count(C_{mars})-Count(C_{origin})}{Count(C_{Origin})}
$$
为了让增加或者减少的部分不至于影响到汉字的主体部分,需要对其进行限制.这里选择的过滤条件是增加或者减少的部分不得超过50%.
$$
Filter(C,score)= \begin{cases}
  True& \text{ if } score<0\ and\ score>-0.5 \\
  True& \text{ if } score>0\ and\ score<1 \\
  False&\ Other\ case
\end{cases}
$$



之后对过滤后的汉字得分取绝对值,取得分最小的汉字即为最接近的异体字.
$$
C_{target}=Sort(Filter(MarsCharList[1..n],score))[0]
$$

#### 动态生成形近字

##### 以同偏旁汉字为切入口,通过递归方法计算汉字的结构加权相似度.利用动态规划的思想加速汉字相似度比对

对于给定的汉字C,攻击模块会在反向字典中搜索和该汉字共享同一偏旁部首的汉字作为形近替换的候选对象.在获得候选列表后使用汉字相似度递归比较算法获得最相似的汉字作为形近字攻击的结果返回.

该算法主要是利用汉字结构二叉树进行了计算.对于要比较相似度的两个汉字,比较他们的结构是否一致.对于不一致的结构,若其中一个是独体字或者组合字则直接认为他们不相似,返回0作为相似度.否则计算结构枚举的差作为结构偏差系数,然后作为结构一致的计算.
对于结构一致的情况如果两者都是独体字,就去直接计算他们的相似度并返回上一级.否则,分别递归计算两个子部件的相似度,然后按笔画数占比求和,乘以结构偏差系数得到两个汉字关于该节点的相似度.      

计算完成后排序选取相似度最大的.

```python
def char_sim_recure(c1:Hanzi,c2:Hanzi)->float:
	if c1 or c2 is Indivisible:
		if c1 or c2 is Char List:
			return 0.0
		else:
			return Char_sim(c1,c2)

	left_score  = char_sim_recure(c1[0],c2[0]) * c1[0].count
	right_score = char_sim_recure(c1[1],c2[1]) * c1[1].count

	all_score   = left_score + right_score
	all_ccore   = all_score / (c1.count * cal_stru_sim(c1.stru,c2.stru))

	return all_score
```

至于具体计算两个部件相似程度的方法Char_sim,前人有使用人工的方式去一一给部件标注相似度^^,但是根据估算大约有4万个工作量,所以予以放弃.本文采取的策略是利用文字渲染模块渲染汉字部件,然后通过简易的向量相似度比较.

相较于使用CNN及其他图像神经网络进行形近字比对的攻击方法,本攻击的创新点在于利用了动态规划的思想,将上万个汉字的一一比较转化为了数百个汉字部件的比较.使得其不管是在运行速度上还是占用内存上都大大降低.同样的,在后期也可以将汉字部件的比较接入CNN孪生神经网络实现更加精确的比较.



文字渲染模块

文字渲染模块会模拟一个对字符集的显示支持为现在广泛通行的 Unicode 8.0版本的文字显示终端,用来渲染攻击后的文本.同时模块还提供了渲染图像向Ndarray向量的转换接口和一些简单的图像相似度比较接口.

由于True Type Font(即ttf格式)的格式限制,一个.ttf字体文件中不可能储存比uint16_t(65535)更多的字体图像https://learn.microsoft.com/en-us/typography/opentype/spec/ .然而仅基础汉字区也就是CJK兼容表意区(U+4E00-U+9FA5)就有20645个汉字,如果全部收录会大大挤压其他字符的收录空间,如:英语,法语和拉丁字母,标点符号和emoji等.因此,即使是抛弃了对其他字符的支持,单一字体也很难实现汉字字符的全收集.

对于这一点,一般字体提供者的解决方案是将一个字体文件拆分成多个字体文件分别储存注册到操作系统中.然后由操作系统将默认字体中不存在的字映射到指定的存在该字的字体中.本文的解决方案与此类似,渲染模块使用宋体字体为基础字体尝试渲染,对于不能渲染的字体,之后会依次遍历其他系统自带字体尝试渲染.对于搜索完毕仍没有对应字形的,则认定该字符不适合作为攻击目标,更换为下一个字符.这样就可以基本上解决OOV问题。

综上所述，Text2Img模块使模型的输⼊与标消息接收者看到的视觉信息保持⼀致，即该模块确保分类器有机会捕获垃圾邮件发送者想要传播的任何语义。



#####



### 以Unicode为核心的形态学攻击



现在通行的字符编码是Unicode码,其中主要以UTF-8编码为主.统一码（Unicode）.

Unicode是⼀种字符集，旨在统一所有字符集,将所有字符用同一套编码完成。目前,Unicode标准已经推出到Unicode 15.0.0.不管是中文,英文,数学符号还是emoji,在 Unicode 中都可以被编码成⼀个十六进制字符串加上前缀"U+"表示,如字符"一"的编码是U+4E00。Unicode只负责分配编码,并不限制机内码的具体编码方式,具体的实现有UTF-8,UTF-16,UTF-32等等.其中最常见也是最常用的编码方式是UTF-8,目前该方式占据了互联网上超过97.9%的编码方式,是当之无愧的主流方式.这是⼀种可变长度编码方案，将代码点表示为1-4个字节。
https://w3techs.com/technologies/cross/character_encoding/ranking

#### 同形异码字

##### 使用Unicode中具有类似形态但是编码不同的汉字作为替换的对象进行攻击

由于历史原因,东亚各国也就是中日韩(下简称CJK)都制定了自己的字符集,对于同一个汉字的编码也有所不同.对此Unicode制定了表意文字认同原则和字源分离原则.前者要求同一个具有相同意义,可以被认为是同一个的汉字,分配同一个编码.如下图中的行,尽管他在不同国家的环境下具有不同的字形,但是在Unicode中仍为同一个字符(U+884C)

![img](C:U+sers\abget\Study\黑盒攻击\代码部分\2685374-b6a52716d60f3f40.webp).

这样做无疑大大减少同形字符出现的可能.但是,Unicode又制定了字源分离原则,即如果两个字符的意思和来源不同,仅仅是字形相同,那么仍给与两个字不同的编码.又如韩国字符集EUC-KR对同字不同音的汉字也给与了不同的编码,为了和已有的字符集兼容,Unicode也不得不将这些字加入因此,有大量的同形字符获得了不同的编码.这些字符对于一般的训练模型来说很难接触到,属于OOV.

![](C:U+sers\abget\Study\黑盒攻击\代码部分\⻳龟-1681013346233-2.png)

解释解释OOV.

尤其是一般的字体为了方便起见,对于那些同形异码字,多半会选择直接复制对应同形字的字形,而不是选择绘制一个不同的字形.因此在一般情况下,该字符的替换不会对视觉阅读造成任何影响.

#### 字符插入攻击

##### 选择日语平假名片假名等对汉语认读影响不大的字符进行插入.

因为如阿拉伯语等语言的文字会随着需要加上对应的修饰符,Unicode中存在一些可以附加到其他字符上的字符也就是拼音和音调字符,U+0300到U+036f.

另外,Unicode也存在一些不可见字符,包括控制字符(U+0000-U+001F),零宽字符(U+200B,U+200C,U+200D,U+2060,U+FEFF),方向控制字符(U+200E,U+200F,U+202C),不会生成呈现的字形，但它们仍然代表有效的编码字符。基于文本的NLP模型对编码字节作为输⼊进行操作，因此这些字符将被基于文本的模型“看到”，即使它们没有呈现给⼈类用⼾可感知的任何东西。我们发现这些字节改变了模型输出。当任意注⼊模型的输⼊时，它们通常会降低准确性和运行时间方⾯的性能。当以有针对性的方式注⼊时，它们可用于以所需方式修改输出，并可能连贯地改变许多NLP任务中输出的含义

#### Unicode顺序攻击

由于阿拉伯文是从右向左写的,而通行的语言是从左向右书写的.因此Unicode规定了字符的方向性,若字符为英语中文等语言则从左向右显示,若为阿拉伯文则从右向左显示.另一方面Unicode提供了不可显示的标识字符方向性的字符(U+202A,U+202B,U+202D,U+202E)通过标识字符,可以让后面的字符强制按照指定方向显示.同时Unicode也提供了相应的解析算法即Bidi算法.可以反向利用该算法,将汉字的方向进行调换.这样,虽然文本显示仍为原来的样子,但实际上的字符序列已经完全不同了.为了让攻击效果尽可能的大,这里使用了混合顺序攻击算法.

首先为方便起见,用空格将文本长度补齐成偶数.因为方向控制字符不可显示,因此将其与后面一位的可显示字符为同一个字符,用⊕标识连接.
$$
f(i)=\begin{cases}
	U+202A\oplus Char_i&if\  i\%2==1\\
	U+202E\oplus Char_{SentenceLen-i+1}&else
\end{cases}
$$


这样即使文本处理模型滤掉了控制字符,剩下的文本仍然是不可以解读的.

![](C:\Users\abget\Study\黑盒攻击\代码部分\反向算法.png)

#### Unicode emoji攻击

有些emoji可以替换相应的名词,因为现在网络emoji大量流行,可以选取一些形象的emoji对汉字进行替换,例如🀄可以替换中字,🈲替换禁字.

![image-20230409121435798](C:U+sers\abget\Study\黑盒攻击\代码部分\image-20230409121435798-1681013684672-4.png)

一个合理的选择是直接衡量去除ith字的效果，因为比较去除一个字之前和之后的预测结果可以反映出这个字对分类结果的影响，如图3所示。因此，我们引入了一个评分，确定x中jth词的重要性为。Cwj=Fy(w1,w2,--,wm)-Fy(w1,--,wj-1,wj+1,--,wm)（3）。
拟议的评分函数具有以下特性。
(1)它能够正确地反映单词对预测的重要性，(2)它在不知道分类模型的参数和结构的情况下计算单词的分数，并且(3)它的计算效率高

## 实验结果

我们使用四个指标，即编辑距离、雅卡德相似系数、欧氏距离和语义相似度，来评估生成的对抗性文本的效用。具体来说，编辑距离和Jaccard相似系数是在原始文本上计算的，而欧氏距离和语义相似度是在单词向量上计算的。
编辑距离。编辑距离是通过计算将一个字符串转换为另一个字符串所需的最小操作数来量化两个字符串（例如，句子）的不同程度的方法。具体来说，编辑距离的不同定义使用不同的字符串操作集。在我们的实验中，我们使用最常见的度量，即列文斯坦距离，其操作包括删除、插入和替换字符串中的字符。
雅卡德相似性系数。雅卡德相似系数是一个用于测量有限样本集的相似性和多样性的统计数字。它被定义为交集的大小除以样本集的联合体的大小。

们用词的移动距离WMD来衡量对抗性例子的质量。WMD表示从句子向量p中的词pi到句子向量q中的词qj的整体移动距离，其中Ti,j是移动距离的权重，d是向量之间的欧氏距离，因此WMD可以表述为：。



### 多模型运行结果

验证结果依次为
2000个的准确度下降程度运行时间比较
bert.zh~亚马逊
Structbert-tiny~豆瓣轮胎
Paddle-nano~paddle

拆分攻击方式再来一遍,在亚马逊数据集上做测试.

零影响替换~bidi算法攻击,Unicode同形码攻击和空白插入攻击,emoji替换
低影响替换~汉字拆解攻击和汉字增减攻击
高影响替换~形近字替换攻击

判断指标,对于零影响的不做判断,对于低和高影响的用问卷判断,找10个人.

用贝叶斯模型和不用贝叶斯模型进行判断

随机和词重要性比较

和PWW模型做为基线模型进行比较?

![img](https://pic3.zhimg.com/v2-2891cdff9038c659d54c84aee6eba6ca_r.jpg)



表二总结了IMDB和MR数据集上白盒攻
击的主要结果和基线方法的性能比较，其中表二的第三列显示了非对抗性设置下的原始模型准确性。我们没有给出在白盒设置下生成一个对抗性例子的平均时间，因为模型是离线的，而且攻击的效率很高（例如，在一秒钟内生成数百个对抗性文本）。从表二中，我们可以看到，随机选择要改变的词（即表二中的随机）对最终结果几乎没有任何影响。这意味着随机改变单词不会欺骗分类器，选择重要的单词进行修改是成功攻击的必要条件。从表二中，我们还可以看到，标模型在非对抗性设置中都表现得相当好。然而，由TEXTBUGGER生成的对抗性文本对这些模型的攻击成功率仍然很高。此外，线性模型比深度学习模型更容易受到对抗性文本的影响。具体来说，TEXTBUGGER只扰动了几个词就达到了很高的攻击成功率，并且对所有
模型的表现都比基线算法好很多，如表二所示。例如，在IMDB数据集上针对LR模型实现95.2%的成功率时，它只扰乱了一个样本的4.9%的单词，而所有基线在这种情况下实现的成功率不超过42%。由于IMDB数据集的平均长度为215.63个单词，TEXTBUGGER只对一个样本进行了约10个单词的扰动，就能进行成功的攻击



## 总结和展望

### 优势

### 不足

### 继续努力的方向

[[confusables](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#confusables)]中的数据提供了一种机制，用于确定两个字符串何时在视觉上容易混淆。随着时间的推移，这些文件中的数据可能会得到完善和扩展。有关随时间推移处理修改的信息，请参阅Unicode技术报告#36“Unicode安全注意事项”[[UTR36\]中的](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UTR36)*第2.9.1节“向后兼容性*”和本文档的[“迁移”部分。](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#Migration)

收集用于检测网守易混淆字符串的数据前不是本文档中易混淆检测机制的标。有关详细信息，请参阅第2节，[[UTR36\]中的](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UTR36)*视觉安全问题*。

数据提供了从源字符到原型的映射。原型应该被认为是一个或多个符号类别的序列，其中每个类别都有一个示例字符。例如，字符U+0153(œ)，拉丁小写连字OE，其原型由两个符号类组成：一个是示例字符U+006F(o)，另一个是示例字符U+0065(e).如果输入字符没有在数据文件中明确定义的原型，则假定原型由以输入字符作为示例字符的符号类组成。

对于输入字符串X，将[骨架](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#def-skeleton)(X)定义为字符串的以下转换：

1. [将X转换为NFD格式，如[UAX15](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UAX15)]中所述。
2. 根据指定数据连接X中每个字符的原型，生成一串示例字符。
3. 重新申请NFD。

当且仅当skeleton(X)=skeleton(Y)时，字符串X和Y被定义为可[混淆。](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#def-confusable)这缩写为X≅Y。

这种机制对数据施加了传递性，因此如果X≅Y和Y≅Z，则X≅Z。可以通过提供给定字符之间的度量来提供更复杂的混淆检测，指示它们的“接近度”。然而，这在计算上要昂贵得多，并且需要更复杂的数据，因此此时选择了更简单的机制。这意味着在某些情况下，测试可能过于包容。



Unicode标准提供可用于确定字符脚本和检测混合脚本文本的信息。脚本的确定是根据*UAX#24，Unicode脚本属性*[[UAX24](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UAX24)]，使用来自Unicode字符数据库[[UCD](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UCD)]的数据。

通过以下两个修改将角色的[扩充脚本集定义为角色的Script_Extensions。](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#def-augmented-script-set)

1. 包含多个脚本的书写系统的条——Hanb（带有Bopomofo的汉字）、Jpan（日语）和Kore（韩语）——是根据以下规则添加的。
    1. 如果Script_Extensions包含Hani(Han)，请添加Hanb、Jpan和Kore。
    2. 如果Script_Extensions包含Hira（平假名），请添加Jpan。
    3. 如果Script_Extensions包含假名（片假名），请添加Jpan。
    4. 如果Script_Extensions包含Hang(Hangul)，请添加Kore。
    5. 如果Script_Extensions包含Bopo（Bopomofo），则添加Hanb。
2. 包含Zyyy（通用）或Zinh（继承）的集合被视为**ALL**，即所有脚本值的集合。

Script_Extensions数据来自Unicode字符数据库[[UCD](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UCD)]。有关Script_Extensions属性和Jpan、Kore和Hanb的更多信息，请参阅*UAX#24，Unicode脚本属性*[[UAX24](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#UAX24)]。

将字符串[的解析脚本集](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#def-resolved-script-set)定义为字符串中所有字符的扩充脚本集的交集。

如果解析的脚本集为空，则字符串定义为[混合脚本；](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#def-mixed-script)如果解析的脚本集为非空，则字符串定义为[单脚本。](https://www-unicode-org.translate.goog/reports/tr39/tr39-22.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN#def-single-script)

请注意，术语“*单*脚本字符串”可能会造成混淆。*这意味着解析的脚本集中至少*有一个脚本，而不是只有*一个*.例如，字符串“〆切”是单文字，因为它在解析的文字集中有*四个*文字{Hani,Hanb,Jpan,Kore}。

参考文献区

https://www.unicode.org/reports/tr39/tr39-22.html

https://www.unicode.org/reports/tr36/tr36-15.html